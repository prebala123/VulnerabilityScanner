# Command line flags
# -a <API key>
# -m <model like "gpt-3.5-turbo">
# -b <True or False for if base images vulnerabilities should be included, default True>
# -j <folder for JSON files>

# Import packages
import openai
import tiktoken
import json
import re
import numpy as np
import pandas as pd
import sys
import subprocess
import os
import time
import argparse

# Get arguments from command line
parser = argparse.ArgumentParser(description="Get command-line arguments.")
parser.add_argument('--apikey', '-a', type=str, required=True, help="API key")
parser.add_argument('--model', '-m', type=str, required=True, help="Model")
parser.add_argument('--includeBase', '-b', type=str, required=False, help="Include base image vulnerabilities")
parser.add_argument('--JSONfolder', '-j', type=str, required=True, help="JSON files")
args = parser.parse_args()

# Set up global variables
API_KEY = args.apikey
model = args.model
folder = args.JSONfolder
if args.includeBase:
    includeBase = args.includeBase
else:
    includeBase = "True"
openai.api_key = API_KEY
limits = {'gpt-3.5-turbo':4096, 'gpt-3.5-turbo-16k':16384}
token_limit = limits[model]

#Prompts for Chat GPT API
system_prompt_neo4j = """
The following Software Bill of Materials contains security vulnerabilities for different components. Each vulnerability is stored in an object in the array "matches". Each vulnerability in the json stores the component name in "artifact.name", the current version in "artifact.version", the severity in "vulnerability.severity", and whether there is a fix in "vulnerability.fix.state". The fix version if it exists is in "vulnerability.fix.versions" Only use vulnerabilities under the tag "vulnerability" and do not use "relatedVulnerabilities". Read the file and create the following JSON output:
1. Create a string representation of a json object containing all of the information which has the format '{"vulnerabilities":[{"component":,"vulnerabilityId":,"severity":,"currentVersion":,"fixVersion":}]}'
2. Make sure the JSON is able to be parsed and is a standard JSON format including proper start and end brackets.
3. Do not include any other output except for the JSON as a string.
The SBOM file is:
"""

system_prompt_cyclone = """
The following Software Bill of Materials in the Cyclonedx format contains security vulnerabilities for different components. Each vulnerability is stored in an object in the array "vulnerabilities". Each vulnerability in the json stores the component name in "affects.ref.component_name", the id in "id", the current version in "affects.ref.current_version", the severity in "ratings.severity", and the fix version. The field "affects.ref" has the format "pkg:<type>/<namespace>/<component name>@<current version>?<coordinates>". Only use the field given in the above instructions and put "NA" if it is unknown. Read the file and create the following JSON output: 
1. Create a string representation of a json object containing all of the information which has the format '{"vulnerabilities":[{"component":,"vulnerabilityId":,"severity":,"currentVersion":,"fixVersion":}]}'
2. Make sure the JSON is able to be parsed and is a standard JSON format including proper start and end brackets.
The SBOM file is:
"""
    
summary_prompt = """
The following JSON contains security vulnerabilities for different components. The format of the JSON is an array with vulnerability objects that contain the component in 'component', rank in 'rank', current version in 'currentVersion', and fix version in 'fixVersion'. Create a summary of each component which contains the score and fix version for each vulnerability. The output should have this format for each component: "<component>: Severity score <rank>.  <number of Critical vulnerabilities> critical vulnerabilities, <number of High vulnerabilities> high vulnerabilities. Update to version <most recent fix version>". Give the output exactly as above and a separate paragraph for each component. Ignore medium, low, and negligible severity. If the input JSON is empty, write "There are no vulnerabilities with fixes". The JSON is:
"""

dtype_mapping = {
    'component': str, 
     'vulnerabilityId': str, 
     'currentVersion': str,
    'fixVersion': str,
    'score': int,
    'file': str
}

def send_ChatGPT(sys_prompt, message):
    """Sends message to ChatGPT and returns the response"""
    response = openai.ChatCompletion.create(
      model=model, 
      messages=[
          {"role": "system", "content": sys_prompt},
          {"role": "user", "content": message}
      ]
    )
    return response

def count_tokens(vulnerability_str):
    """Returns the number of tokens in a text string"""
    encoding = tiktoken.encoding_for_model(model)
    num_tokens = len(encoding.encode(vulnerability_str))
    return num_tokens

def extract_json_from_string(input_string):
    """Use regular expression to find the JSON structure in the Chat GPT response"""
    pattern = r'{ *"vulnerabilities":.*}'
    json_matches = json.loads(re.findall(pattern, input_string.replace('\n',''))[0])['vulnerabilities']
    response_df = pd.DataFrame(json_matches)
    return response_df

def organize_json(sbom):
    """Create body of message for sending vulnerability data to Chat GPT"""
    max_tokens = token_limit - count_tokens(system_prompt)
    out_len = 200
    index = 0
    total = 0
    organized = [[]]
    # Loop through each vulnerability
    for i in sbom:
        tok = count_tokens(json.dumps(i))
        # Check if adding the next vulnerability is within the token limit of the API and add to list if so
        if total + tok + out_len <= max_tokens:
            organized[index].append(i)
            total = total + tok + out_len
        # Otherwise put the vulnerability in the next API message
        else:
            organized[index] = '[' + ','.join([json.dumps(i) for i in organized[index]]) + ']'
            organized.append([i])
            index += 1
            total = tok
    organized[index] = '[' + ','.join([json.dumps(i) for i in organized[index]]) + ']'
    return organized

def update_progress_bar(progress):
    """Set up progress bar for command line"""
    bar_length = 50
    block = int(round(bar_length * progress))
    progress_bar = "[" + "=" * block + " " * (bar_length - block) + "]"
    return progress_bar

def loadJSONfile(filepath):
    """Return the JSON at a given filepath"""
    try:
        sbom = (json.load(open(filepath)))
    except UnicodeDecodeError as e:
        # If the JSON doesn't load, replace certain characters to fit the encoding
        with open(filepath, 'r', encoding='utf-8') as f:
            file_content = f.read()
            fixed_content = file_content.replace("”", "'").replace("“","'")
            sbom = json.loads(fixed_content)
    return sbom

def getImageType(sbom):
    """Find if an image is a base or derived type"""
    imageType = len(sbom['source']['target']['layers'])
    if imageType == 1:
        return "base"
    else:
        return "derived"
    
def getDF(sbom_data, filename):
    """Get a table of vulnerability data from the given JSON"""
    print('\n')
    progress = 0
    progress_bar = update_progress_bar(progress)
    print(f"\rFile: {filename} Progress: {progress_bar} {progress*100:.2f}% complete", end="", flush=True)
    
    data = organize_json(sbom_data)
    scores = {"none":1,"negligible":1,"low":2,"medium":3,"high":4,"critical":5}
        
    outputs = []
    
    # Loop through previously set up Chat GPT messages
    for vul in range(len(data)):
        try:
            # Get table of Vulnerabilities from ChatGPT
            response = send_ChatGPT(system_prompt, json.dumps(data[vul]))
            json_resp = extract_json_from_string(response['choices'][0]['message']['content'])
            output_df = pd.DataFrame(json_resp)
            
            # Clean up columns to make them more usable and fill in missing values
            output_df['severity'] = output_df['severity'].fillna('Negligible')
            output_df['fixVersion'] = output_df['fixVersion'].replace({'':'not-fixed'})
            output_df['score'] = output_df['severity'].str.lower().map(scores).replace({np.nan: 1}).astype(int)
            output_df['file'] = filename
            output_df = output_df[['component', 'vulnerabilityId', 'currentVersion', 'fixVersion', 'score', 'file']]
            output_df = output_df.astype(dtype_mapping)
            outputs.append(output_df)
            
            # Update progress bar on command line
            progress = (vul + 1)/len(data)
            progress_bar = update_progress_bar(progress)
            print(f"\rFile: {filename} Progress: {progress_bar} {progress*100:.2f}% complete", end="", flush=True)
        except:
            continue
        
    return outputs

start = time.time()

# Dictionary to store vulnerability dataframes of images
base_images = []
derived_images = []

# Iterate through each JSON file in the folder and determine what type they are
for filename in os.listdir(folder):
    if filename.endswith(".json"):
        file_path = os.path.join(folder, filename)
                
        data = loadJSONfile(file_path)
        try:
            image_type = getImageType(data)
        except:
            continue
        
        if image_type == 'base':
            base_images.append(filename)
        else:
            derived_images.append(filename)

base_vulnerabilities = {}
system_prompt = system_prompt_neo4j

# Iterate through base images
for filename in base_images:
    file_path = os.path.join(folder, filename)

    # Save all vulnerabilities of base images in a dictionary
    data = loadJSONfile(file_path)
    image_type = getImageType(data)
    sbom_data = data['matches']
    layer = data['source']['target']['layers'][0]['digest']
    try:
        baseOutput = pd.concat(getDF(sbom_data, filename))
    except:
        baseOutput = pd.DataFrame(columns=['component', 'vulnerabilityId', 'currentVersion', 'fixVersion', 'score', 'file'])
    
    base_vulnerabilities[layer] = baseOutput
    
derived_vulnerabilities = []

# Iterate through derived images
for filename in derived_images:
    file_path = os.path.join(folder, filename)

    # Get the vulnerabilities of the images
    data = loadJSONfile(file_path)
    image_type = getImageType(data)
    sbom_data = data['matches']
    layer = data['source']['target']['layers'][0]['digest']
    try:
        derivedOutput = pd.concat(getDF(sbom_data, filename))
    except:
        derivedOutput = pd.DataFrame(columns=['component', 'vulnerabilityId', 'currentVersion', 'fixVersion', 'score', 'file'])
    onlyDerived = derivedOutput
    
    # Delete vulnerabilities that already exist in the base image
    try:
        if includeBase != "True":
            baseOutput = base_vulnerabilities.get(layer, pd.DataFrame(columns=['component', 'vulnerabilityId', 'currentVersion', 'fixVersion', 'score', 'file']))
            mask = ~derivedOutput[['component', 'vulnerabilityId']].isin(baseOutput[['component', 'vulnerabilityId']]).all(axis=1)
            onlyDerived = derivedOutput[mask]
    except:
        pass
    
    derived_vulnerabilities.append(onlyDerived)
    
# Combine vulnerabilities from all images
try:
    output_dfraw = pd.concat(derived_vulnerabilities).reset_index(drop=True)
except:
    output_dfraw = pd.DataFrame(columns=['component', 'vulnerabilityId', 'currentVersion', 'fixVersion', 'score', 'file'])

if includeBase == "True":
    try:
        base_df = pd.concat(base_vulnerabilities.values()).reset_index(drop=True)
    except:
        base_df = pd.DataFrame(columns=['component', 'vulnerabilityId', 'currentVersion', 'fixVersion', 'score', 'file'])
    output_dfclean = output_dfraw.astype(dtype_mapping)
    output_df = pd.concat([output_dfclean, base_df]).reset_index(drop=True).drop_duplicates()
else:
    output_df = output_dfraw.astype(dtype_mapping)

# Clean up output and get severity scores
scores = {1:"Negligible",2:"Low",3:"Medium",4:"High",5:"Critical"}
output_df['severity'] = output_df['score'].map(scores)
output_df['fixVersion'] = output_df['fixVersion'].fillna('not-fixed')
clean_output_df = output_df[-(output_df['fixVersion'].isin(['wont-fix', 'not-fixed', 'NA', 'N/A', '', 'unknown', 'Not fixed', 'Not Fixed', '[]', 'False', 'None', 'none']))]

# Increase score for components in multiple images
count_summary = clean_output_df[['component','file']].drop_duplicates().groupby('component').count()
ranks = pd.DataFrame(clean_output_df.groupby('component')['score'].sum()).rename(columns={'score':'rank'}).sort_values('rank', ascending=False)
file_summary = pd.DataFrame(count_summary.merge(ranks, left_index=True, right_index=True).product(axis=1)).rename(columns={0:'rank'}).sort_values('rank', ascending=False).astype(int)
merged_df = clean_output_df.merge(file_summary, on='component').sort_values(['rank', 'component', 'score'], ascending=False).reset_index(drop=True)

# Get the top 5 most vulnerable components to display
most_vulnerable_components = list(file_summary.index[0:5])
most_vulnerable_df = merged_df[merged_df['component'].isin(most_vulnerable_components)]
final_df = most_vulnerable_df.drop(columns=['score'])
most_vulnerable_json = most_vulnerable_df.drop(columns=['score']).to_json(orient='records')

end = time.time()
elapsed = end - start

# Print the top 5 most vulnerable components, with the JSON and table format, to command line
final_output = send_ChatGPT(summary_prompt, json.dumps(final_df.to_json(orient='records')))
content = final_output['choices'][0]['message']['content']
outputstr = (
        f"\n\nCompleted in {elapsed:.2f} seconds\n\n\n\n" +
        "These are the top 5 most vulnerable components:\n\n" +
        content + 
        "\n\nThis is the json output:\n\n" + 
        str(merged_df.drop(columns=['score','rank','file']).drop_duplicates().to_json(orient='records')) +
        "\n\nThis table contains all of the vulnerabilities:\n\n" + 
        str(merged_df.drop(columns=['score','rank','file']).drop_duplicates().reset_index(drop=True)) + 
        "\n\n"
    )

print(outputstr)